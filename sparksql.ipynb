{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79c90f6e-538b-4819-9610-38992ca06ad4",
   "metadata": {},
   "source": [
    "##Import library needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00b45745-b7d4-4f6a-b069-d55592f7c6dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2391dfe2-f7f7-462f-a785-0bd5c1cd121c",
   "metadata": {},
   "source": [
    "#Create Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31dcb65f-32e4-4e65-bd60-08cf14cf4ace",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/20 00:01:30 WARN Utils: Your hostname, Bowens-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.0.118 instead (on interface en0)\n",
      "23/02/20 00:01:30 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/20 00:01:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/02/20 00:01:34 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "#Create a spark session\n",
    "spark = SparkSession.builder.appName(\"SparkSQL\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e93635fe-d382-45a1-9756-76800acf5967",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:===================================================>     (10 + 1) / 11]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is our inferred schema:\n",
      "root\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropOff_datetime: timestamp (nullable = true)\n",
      " |-- PUlocationID: integer (nullable = true)\n",
      " |-- DOlocationID: integer (nullable = true)\n",
      " |-- SR_Flag: integer (nullable = true)\n",
      " |-- Affiliated_base_number: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\")\\\n",
    "    .csv(\"fhv_tripdata_2019-01.csv\")\n",
    "print(\"Here is our inferred schema:\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f22dc0d-f9d5-4d92-a1e3-0ff2fc44f5cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropOff_datetime|PUlocationID|DOlocationID|SR_Flag|Affiliated_base_number|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|              B00001|2019-01-01 00:30:00|2019-01-01 02:51:55|        null|        null|   null|                B00001|\n",
      "|              B00001|2019-01-01 00:45:00|2019-01-01 00:54:49|        null|        null|   null|                B00001|\n",
      "|              B00001|2019-01-01 00:15:00|2019-01-01 00:54:52|        null|        null|   null|                B00001|\n",
      "|              B00008|2019-01-01 00:19:00|2019-01-01 00:39:00|        null|        null|   null|                B00008|\n",
      "|              B00008|2019-01-01 00:27:00|2019-01-01 00:37:00|        null|        null|   null|                B00008|\n",
      "|              B00008|2019-01-01 00:48:00|2019-01-01 01:02:00|        null|        null|   null|                B00008|\n",
      "|              B00008|2019-01-01 00:50:00|2019-01-01 00:59:00|        null|        null|   null|                B00008|\n",
      "|              B00008|2019-01-01 00:51:00|2019-01-01 00:56:00|        null|        null|   null|                B00008|\n",
      "|              B00009|2019-01-01 00:44:00|2019-01-01 00:58:00|        null|        null|   null|                B00009|\n",
      "|              B00009|2019-01-01 00:19:00|2019-01-01 00:36:00|        null|        null|   null|                B00009|\n",
      "|              B00009|2019-01-01 00:36:00|2019-01-01 00:49:00|        null|        null|   null|                B00009|\n",
      "|              B00009|2019-01-01 00:26:00|2019-01-01 00:32:00|        null|        null|   null|                B00009|\n",
      "|              B00009|2019-01-01 00:26:00|2019-01-01 00:36:00|        null|        null|   null|                B00009|\n",
      "|              B00009|2019-01-01 00:58:00|2019-01-01 01:05:00|        null|        null|   null|                B00009|\n",
      "|              B00013|2019-01-01 00:02:29|2019-01-02 00:25:30|        null|        null|   null|                B00013|\n",
      "|              B00013|2019-01-01 00:01:33|2019-01-02 00:18:16|        null|        null|   null|                B00013|\n",
      "|              B00037|2019-01-01 00:02:43|2019-01-01 00:10:14|        null|         265|   null|                B00037|\n",
      "|              B00037|2019-01-01 00:26:02|2019-01-01 00:37:00|        null|         265|   null|                B00037|\n",
      "|              B00037|2019-01-01 00:11:16|2019-01-01 00:25:41|        null|         265|   null|                B00037|\n",
      "|              B00037|2019-01-01 00:33:45|2019-01-01 00:45:28|        null|         265|   null|                B00037|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "359006ac-0797-4de6-800c-1d788ea4a939",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"tripData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ab3fa84-2e45-4974-a929-90a6cd726797",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:===============================================>          (9 + 2) / 11]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+----------------+\n",
      "|dispatching_base_num|pickup_datetime|dropoff_datetime|\n",
      "+--------------------+---------------+----------------+\n",
      "+--------------------+---------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlDF = spark.sql(\"SELECT DISTINCT dispatching_base_num,  pickup_datetime, dropoff_datetime FROM \\\n",
    "tripData where PULocationID is not null and DOLocationID ='264' and dispatching_base_num= 'B02182' \\\n",
    "GROUP BY dispatching_base_num,  pickup_datetime, dropoff_datetime ORDER BY pickup_datetime ASC\")\n",
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22dc5f21-03f3-4c42-aa0d-f9a977992b21",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:===================================================>     (10 + 1) / 11]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|SR_Flag|\n",
      "+-------+\n",
      "|     26|\n",
      "|     27|\n",
      "|     12|\n",
      "|     22|\n",
      "|   null|\n",
      "|      1|\n",
      "|     13|\n",
      "|      6|\n",
      "|     16|\n",
      "|      3|\n",
      "|     20|\n",
      "|      5|\n",
      "|     19|\n",
      "|     15|\n",
      "|      9|\n",
      "|     17|\n",
      "|      4|\n",
      "|      8|\n",
      "|     23|\n",
      "|      7|\n",
      "+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_distinct = spark.sql(\"SELECT DISTINCT SR_Flag from tripData where PULocationID IS NOT NULL and DOLocationID is NOT NULL \")\n",
    "df_distinct.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21ec99a3-fed4-4102-a149-da8220e8088a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sqlDF.write.parquet(\"spark_write_parquet.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "70a93a3e-dc3d-4f14-912f-8ab677a19f0c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropoff_datetime|\n",
      "+--------------------+-------------------+-------------------+\n",
      "|              B02182|2020-04-01 00:15:44|2020-04-01 00:43:22|\n",
      "|              B02182|2020-04-01 04:22:56|2020-04-01 04:30:10|\n",
      "|              B02182|2020-04-01 04:44:10|2020-04-01 05:06:28|\n",
      "|              B02182|2020-04-01 05:04:45|2020-04-01 05:11:29|\n",
      "|              B02182|2020-04-01 05:16:59|2020-04-01 05:49:06|\n",
      "|              B02182|2020-04-01 05:23:25|2020-04-01 05:32:03|\n",
      "|              B02182|2020-04-01 05:39:37|2020-04-01 05:42:09|\n",
      "|              B02182|2020-04-01 05:46:15|2020-04-01 06:17:27|\n",
      "|              B02182|2020-04-01 05:57:29|2020-04-01 06:22:49|\n",
      "|              B02182|2020-04-01 05:59:38|2020-04-01 06:07:01|\n",
      "|              B02182|2020-04-01 06:14:17|2020-04-01 06:36:56|\n",
      "|              B02182|2020-04-01 06:33:18|2020-04-01 07:13:58|\n",
      "|              B02182|2020-04-01 06:41:25|2020-04-01 07:13:00|\n",
      "|              B02182|2020-04-01 06:54:24|2020-04-01 07:03:41|\n",
      "|              B02182|2020-04-01 06:59:33|2020-04-01 07:38:25|\n",
      "|              B02182|2020-04-01 07:00:40|2020-04-01 07:14:23|\n",
      "|              B02182|2020-04-01 07:00:44|2020-04-01 07:48:54|\n",
      "|              B02182|2020-04-01 07:07:46|2020-04-01 07:07:54|\n",
      "|              B02182|2020-04-01 07:15:25|2020-04-01 07:37:02|\n",
      "|              B02182|2020-04-01 07:28:34|2020-04-01 07:53:51|\n",
      "+--------------------+-------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_parquet = spark.read.parquet(\"spark_write_parquet.parquet\")\n",
    "df_parquet.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "240e548a-f001-495c-9c2d-5b65258904cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+------------+----------------------+\n",
      "|dispatching_base_num|PUlocationID|DOlocationID|Affiliated_base_number|\n",
      "+--------------------+------------+------------+----------------------+\n",
      "|              B00001|         264|         264|                B00001|\n",
      "|              B00008|         264|         264|                B00008|\n",
      "|              B00009|         264|         264|                B00009|\n",
      "|     B00021         |         260|         226|       B00021         |\n",
      "|     B00021         |          82|          82|       B00021         |\n",
      "|     B00021         |          82|         129|       B00021         |\n",
      "|     B00021         |          70|         129|       B00021         |\n",
      "|     B00021         |         173|          70|       B00021         |\n",
      "|     B00021         |         129|         138|       B00021         |\n",
      "|     B00021         |          56|          82|       B00021         |\n",
      "|     B00021         |         129|         173|       B00021         |\n",
      "|     B00021         |          82|          95|       B00021         |\n",
      "|     B00021         |           7|         223|       B00021         |\n",
      "|     B00021         |         173|         157|       B00021         |\n",
      "|     B00021         |           7|           7|       B00021         |\n",
      "|     B00021         |          56|         173|       B00021         |\n",
      "|     B00021         |         173|          56|       B00021         |\n",
      "|     B00021         |          95|         173|       B00021         |\n",
      "|     B00021         |         129|         226|       B00021         |\n",
      "|     B00021         |          56|          56|       B00021         |\n",
      "+--------------------+------------+------------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlTripData = spark.sql(\"SELECT dispatching_base_num, PUlocationID, DOlocationID, Affiliated_base_number \\\n",
    "FROM tripData WHERE PULocationID IS NOT NULL and DOLocationID is NOT NULL AND pickup_datetime >= '2020-04-01' \\\n",
    "AND pickup_datetime <='2020-04-03' GROUP BY dispatching_base_num, PUlocationID, DOlocationID, Affiliated_base_number \\\n",
    "ORDER BY dispatching_base_num\")\n",
    "sqlTripData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86751e30-480d-4386-b5f7-dc25d19a4a54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
